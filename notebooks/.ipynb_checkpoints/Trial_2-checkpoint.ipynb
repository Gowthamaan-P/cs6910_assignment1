{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c104846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "366386d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "    \n",
    "    def get_loss(self, y, y_hat):\n",
    "        if self.method == \"cel\":\n",
    "            return self.cross_entropy_loss(y, y_hat)\n",
    "        elif self.method == \"mse\":\n",
    "            return self.mean_square_error(y, y_hat)\n",
    "    \n",
    "    def get_derivative(self, y, y_hat):\n",
    "        if self.method == \"cel\":\n",
    "            return self.cross_entropy_loss_derivative(y, y_hat)\n",
    "        elif self.method == \"mse\":\n",
    "            return self.mean_square_error_derivative(y, y_hat)\n",
    "    \n",
    "    def mean_square_error(self, y, y_hat):\n",
    "        return np.sum((y - y_hat) ** 2) / 2\n",
    "    \n",
    "    def mean_square_error_derivative(self, y, y_hat):\n",
    "        return y_hat - y\n",
    "    \n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        return -np.sum(y * np.log(y_hat))\n",
    "    \n",
    "    def cross_entropy_loss_derivative(self, y, y_hat):\n",
    "        return -y/y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19494b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer: \n",
    "    def __init__(self, index, n_input, n_neurons, function=None, weights=None, bias=None, method=\"random\"):\n",
    "        self.index = index\n",
    "        self.function = function if function is not None else 'sigmoid'\n",
    "        self.weights = weights if weights is not None else self.initialize_weights(method, n_input, n_neurons)\n",
    "        self.bias = bias if bias is not None else np.random.randn(n_neurons)\n",
    "        self.activation = None\n",
    "        \n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "        \n",
    "        self.d_weights = None\n",
    "        self.d_bias = None\n",
    "\n",
    "    def initialize_weights(self, method, n_input, n_neurons):\n",
    "        if method == \"xavier\":\n",
    "            limit = np.sqrt(2 / (n_input + n_neurons))\n",
    "            return np.random.randn(n_input, n_neurons) * limit\n",
    "        return np.random.randn(n_input, n_neurons)\n",
    "\n",
    "    def activate(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        self.activation = self._apply_activation(z)\n",
    "        return self.activation\n",
    "\n",
    "    def _apply_activation(self, r):\n",
    "        if self.function == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        elif self.function == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        elif self.function == 'relu':\n",
    "            return np.maximum(0, r)\n",
    "        elif self.function == 'softmax':\n",
    "            max_r = np.max(r, axis=1)\n",
    "            max_r = max_r.reshape(max_r.shape[0], 1)\n",
    "            exp_r = np.exp(r - max_r)\n",
    "            return exp_r / np.sum(exp_r, axis=1).reshape(exp_r.shape[0], 1)\n",
    "        return r\n",
    "\n",
    "    def apply_activation_derivative(self, z):\n",
    "        if self.function == 'sigmoid':\n",
    "            return z * (1 - z)\n",
    "        elif self.function == 'tanh':\n",
    "            return (z - z**2)\n",
    "        elif self.function == 'relu':\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif self.function == 'softmax':\n",
    "            return np.diag(z) - np.outer(z, z)\n",
    "        return np.ones(z.shape)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Neural Layer: {self.index}, {self.weights.shape} , {self.function}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ac9d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, config):\n",
    "        def get_value(key, default):\n",
    "            return config[key] if key in config else default\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        self.criterion = get_value('criterion', 'cel')\n",
    "        self.weight_initialization = get_value('weight_initialization', 'random')\n",
    "        \n",
    "        self.c = ObjectiveFunction(method=self.criterion)\n",
    "        \n",
    "        self.add_layers(config['input_size'], \n",
    "                         config['hidden_layers'], \n",
    "                         config['output_size'], \n",
    "                         config['neurons'],\n",
    "                         config['activation'],\n",
    "                         config['output_activation']\n",
    "                        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.activate(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x, y, y_hat):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.error = self.c.get_derivative(y, y_hat)\n",
    "                output_derivative_matrix = []\n",
    "                for i in range(y_hat.shape[0]):\n",
    "                    output_derivative_matrix.append(np.matmul(\n",
    "                        self.c.get_derivative(y[i], y_hat[i]), \n",
    "                        layer.apply_activation_derivative(y_hat[i])\n",
    "                    ))\n",
    "                layer.delta = np.array(output_derivative_matrix)\n",
    "            else:\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.error = np.matmul(next_layer.delta, next_layer.weights.T)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.activation)\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            activation = np.atleast_2d(x if i == 0 else self.layers[i - 1].activation)\n",
    "            layer.d_weights = np.matmul(activation.T, layer.delta)\n",
    "            layer.b_bais = np.sum(layer.delta, axis=0)\n",
    "    \n",
    "    def add_layers(self, input_size, hidden_layers, output_size, neurons, activation, output_activation):\n",
    "        for i in range(0, hidden_layers+1):\n",
    "            n_input = input_size if i==0 else neurons\n",
    "            n_neurons = output_size if i==hidden_layers else neurons\n",
    "            self.layers.append(NeuralLayer(\n",
    "                index=i+1,\n",
    "                n_input=n_input,\n",
    "                n_neurons=n_neurons,\n",
    "                function= output_activation if i==hidden_layers else activation,\n",
    "                method=self.weight_initialization\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "754d0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, nn:NeuralNetwork, config=None):\n",
    "        # Initialize parameters\n",
    "        self.nn, self.lr, self.optimizer = nn, config['learning_rate'], config['optimizer']\n",
    "        self.beta, self.epsilon, self.beta1, self.beta2= config['beta'], config['epsilon'], config['beta1'], config['beta2']\n",
    "        self.timestep = 0\n",
    "        self.decay = config['decay']\n",
    "\n",
    "    def step(self):\n",
    "        if(self.optimizer == \"sgd\"):\n",
    "            self.sgd()\n",
    "    \n",
    "    def sgd(self):\n",
    "        for layer in self.nn.layers:\n",
    "            layer.weights -= self.lr * (layer.d_weights + self.decay*layer.weights)\n",
    "            layer.bias -= self.lr * (layer.d_bias + self.decay*layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7be9b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Input Data\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten the images\n",
    "train_images = train_images.reshape(train_images.shape[0], 784) / 255\n",
    "X_test = test_images.reshape(test_images.shape[0], 784) / 255\n",
    "\n",
    "# Encode the labels\n",
    "train_labels = np.eye(10)[train_labels]\n",
    "Y_test = np.eye(10)[test_labels]\n",
    "\n",
    "# Prepare data for training and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, shuffle=True, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e479aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    train_loss_hist = []\n",
    "    train_accuracy_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_accuracy_hist = []\n",
    "    \n",
    "    nn = NeuralNetwork(config)\n",
    "    optimizer = Optimizer(nn=nn, config=config)\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    criterion = ObjectiveFunction(method = config['criterion'])\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        for batch in range(0, X_train.shape[0], batch_size):\n",
    "            # Get the batch of data\n",
    "            X_batch = X_train[batch:batch+batch_size]\n",
    "            Y_batch = Y_train[batch:batch+batch_size]\n",
    "\n",
    "            Y_hat_batch = nn.forward(X_batch)\n",
    "            nn.backward(X_batch, Y_batch, Y_hat_batch)\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.timestep += 1\n",
    "        \n",
    "        # Training\n",
    "        Y_hat_train = nn.forward(X_train)\n",
    "        train_loss = criterion.get_loss(Y_train, Y_hat_train)\n",
    "        train_accuracy = np.sum(np.argmax(Y_hat_train, axis=1) == np.argmax(Y_train, axis=1)) / Y_train.shape[0]\n",
    "            \n",
    "        # Validation\n",
    "        Y_hat_val = nn.forward(X_val)\n",
    "        val_loss = criterion.get_loss(Y_val, Y_hat_val)\n",
    "        val_accuracy = np.sum(np.argmax(Y_hat_val, axis=1) == np.argmax(Y_val, axis=1)) / Y_val.shape[0]\n",
    "        \n",
    "        print(\"Epoch {} Train Loss {} Train Accuracy {} Val Loss {} Val Accuracy {}\".format(epoch, train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "   \n",
    "        train_loss_hist.append(train_loss)\n",
    "        train_accuracy_hist.append(train_accuracy)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        val_accuracy_hist.append(val_accuracy)\n",
    "    \n",
    "    return nn, train_loss_hist, train_accuracy_hist, val_loss_hist, val_accuracy_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf5dcdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10) (256, 784)\n",
      "(64, 10) (10, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-b3044edd2817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-095320995721>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mY_hat_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_hat_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-34d721edcdcb>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, x, y, y_hat)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_activation_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 10)"
     ]
    }
   ],
   "source": [
    "network_config = {\n",
    "    'input_size': 784,\n",
    "    'output_size': 10,\n",
    "    'hidden_layers': 1,\n",
    "    'neurons':256,\n",
    "    'activation':'sigmoid',\n",
    "    'output_activation':'softmax',\n",
    "    'learning_rate': 0.005,\n",
    "    'beta': 0.8,\n",
    "    'beta1': 0.9,\n",
    "    'beta2':0.9999,\n",
    "    'epsilon': 1e-8,\n",
    "    'epochs': 1,\n",
    "    'optimizer': \"nadam\",\n",
    "    'criterion': \"cel\",\n",
    "    'decay': 0.0005,\n",
    "    'weight_initialization': \"random\",\n",
    "    'batch_size': 64,\n",
    "}\n",
    "        \n",
    "nn, tl, ta, vl, va = train(network_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e6bdb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5,0,- 1):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "152dc0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645d57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
